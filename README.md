# Big Data Processing with Hadoop and MapReduce

## Overview
This project demonstrates my hands-on experience with **Hadoop**, **HDFS**, and **MapReduce** for processing large-scale datasets. Using the **Cloudera VM**, I performed the following tasks, which are showcased in the video tutorial below:
1. **File Manipulation in HDFS**: Uploaded and managed files in Hadoop Distributed File System (HDFS).
2. **MapReduce Programs**: Ran pre-built MapReduce programs (`wordcount` and `wordmean`) to analyze a text dataset.
3. **Custom Python Script**: Developed and executed a Python script to perform word count analysis.

The video provides a step-by-step walkthrough of the entire process, highlighting some Big Data tools and frameworks.

## Tools & Technologies
- **Hadoop Ecosystem**: HDFS, MapReduce
- **Cloudera VM**
- **Python** (for custom word count program)
- **Terminal Commands** (for file manipulation and running Hadoop jobs)

## Video Tutorial
Watch the [video tutorial](video/project_tutorial.mp4) for a detailed demonstration of the project.

## Key Steps Demonstrated in the Video
1. **Setting Up the Environment**:
   - Launching the Cloudera VM and ensuring all services are running.
2. **File Manipulation in HDFS**:
   - Uploading a text dataset (Shakespeare's works) to HDFS.
   - Verifying the file is successfully stored in HDFS.
3. **Running MapReduce Programs**:
   - Executing the `wordcount` program and analyzing the results.
   - Running the `wordmean` program and interpreting the output.
4. **Custom Python Word Count Program**:
   - Developing and running a Python script for word count analysis.

## Dataset
- The dataset used is the **Shakespeare text file**, which is publicly available.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
